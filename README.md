# ğŸ® GameSphere Steam Game Recommender

A complete machine learning pipeline for Steam game recommendations using Apache Spark, collaborative filtering (ALS), BERT-based sentiment analysis, and modern web interfaces.

## ğŸš€ Quick Start

### One-Command Setup
```bash
make setup
```

This will:
- Install all dependencies (Java 11, Python 3.10, Hadoop, Spark)
- Configure HDFS with proper directory structure
- Set up Python virtual environment with all required packages
- Start HDFS services

### Data Pipeline

1. **Put raw data into HDFS:**
```bash
# Download steam_reviews.csv from Google Drive first
make put-raw RAW=/path/to/steam_reviews.csv
```

2. **Run the complete ML pipeline:**
```bash
make run-clean run-sentiment run-als run-export
```

### Launch Services

- **Streamlit Dashboard:** `make streamlit` â†’ http://localhost:8501
- **Airflow Orchestration:** `make airflow-up` â†’ http://localhost:8080
- **MLflow Tracking:** `make mlflow-ui` â†’ http://localhost:5000
- **FastAPI Service:** `cd serving/fastapi && uvicorn main:app --reload` â†’ http://localhost:8000

## ğŸ“Š Architecture

```
Raw Data (CSV) â†’ HDFS â†’ Spark Processing â†’ ML Models â†’ Dashboard/API
     â†“              â†“           â†“             â†“           â†“
Steam Reviews â†’ Data Cleaning â†’ Sentiment â†’ ALS â†’ Streamlit/FastAPI
                     â†“         Analysis   Model      â†“
                English Only      â†“         â†“    Recommendations
                     â†“         BERT NLP     â†“         â†“
                 Parquet       Scoring   User Recs  Visualizations
```

## ğŸ›  Technology Stack

- **Big Data:** Apache Spark 3.5.3, Hadoop 3.4.1, HDFS
- **Machine Learning:** Spark MLlib (ALS), Transformers (BERT), MLflow
- **Orchestration:** Apache Airflow 2.9.2 (Docker)
- **Frontend:** Streamlit, Plotly
- **API:** FastAPI, Uvicorn
- **Infrastructure:** Docker, Python 3.10, Java 11

## ğŸ“ Project Structure

```
GameSphere_Steam_Game_Recommender/
â”œâ”€â”€ AIRFLOW-PROJECT/           # Airflow orchestration
â”‚   â”œâ”€â”€ dags/flow_dag.py      # Main pipeline DAG
â”‚   â”œâ”€â”€ docker-compose.yaml   # Airflow services
â”‚   â””â”€â”€ requirements.txt      # Airflow dependencies
â”œâ”€â”€ dashboards/
â”‚   â””â”€â”€ GameSphere.py         # Streamlit dashboard
â”œâ”€â”€ pipeline/                 # Spark ML pipeline
â”‚   â”œâ”€â”€ clean_reviews.py      # Data cleaning
â”‚   â”œâ”€â”€ run_sentiment.py      # BERT sentiment analysis
â”‚   â”œâ”€â”€ train_als.py          # ALS collaborative filtering
â”‚   â””â”€â”€ export_dashboard_data.py # Final data export
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ bootstrap_macos.sh    # Complete setup script
â”‚   â””â”€â”€ run_mlflow.sh         # MLflow UI launcher
â”œâ”€â”€ serving/fastapi/          # REST API service
â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â””â”€â”€ requirements.txt     # API dependencies
â”œâ”€â”€ Makefile                 # All automation commands
â””â”€â”€ requirements.txt         # Main Python dependencies
```

## ğŸ”§ Manual Setup (Alternative)

If you prefer manual setup or the bootstrap script fails:

### 1. Install Dependencies
```bash
# macOS with Homebrew
brew install openjdk@11 python@3.10 wget git

# Set up directories
mkdir -p ~/Hadoop ~/Spark
```

### 2. Download & Install Hadoop/Spark
```bash
# Hadoop 3.4.1
cd ~/Hadoop
wget https://archive.apache.org/dist/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz
tar -xzf hadoop-3.4.1.tar.gz

# Spark 3.5.3
cd ~/Spark  
wget https://archive.apache.org/dist/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz
tar -xzf spark-3.5.3-bin-hadoop3.tgz
mv spark-3.5.3-bin-hadoop3 spark-3.5.3
```

### 3. Environment Variables
Add to `~/.zshrc`:
```bash
# GameSphere env
export JAVA_HOME=/opt/homebrew/opt/openjdk@11
export PATH="$JAVA_HOME/bin:$PATH"
export HADOOP_HOME=~/Hadoop/hadoop-3.4.1
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export SPARK_HOME=~/Spark/spark-3.5.3
export PYSPARK_PYTHON=python3
export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
```

### 4. Python Environment
```bash
python3.10 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

## ğŸ“Š HDFS Data Layout

```
/project/
â”œâ”€â”€ raw/                                    # Raw CSV files
â”‚   â””â”€â”€ steam_reviews.csv
â”œâ”€â”€ processed/                              # Cleaned data
â”‚   â””â”€â”€ steam_review_english.parquet
â”œâ”€â”€ data/processed/                         # Intermediate results
â”‚   â””â”€â”€ steam_sentiment_summary.parquet
â””â”€â”€ outputs/                               # Final outputs
    â”œâ”€â”€ als_model/                         # Trained ALS model
    â”œâ”€â”€ user_recommendations.parquet       # Per-user recommendations
    â”œâ”€â”€ final_recommendations.parquet      # Final recommendation dataset
    â””â”€â”€ app_recommendations.parquet        # App-level aggregations
```

## ğŸ¯ Pipeline Steps

### 1. Data Cleaning (`clean_reviews.py`)
- Filters null values and invalid records
- Standardizes text formatting
- Extracts English-only reviews
- **Output:** `steam_review_english.parquet`

### 2. Sentiment Analysis (`run_sentiment.py`)
- BERT-based sentiment scoring using Transformers
- Aggregates sentiment by game (app_id)
- Tracks metrics in MLflow
- **Output:** `steam_sentiment_summary.parquet`

### 3. ALS Training (`train_als.py`)
- Collaborative filtering using Spark MLlib ALS
- Implicit feedback from user reviews
- Generates user-item recommendations
- **Output:** `als_model/`, `user_recommendations.parquet`

### 4. Data Export (`export_dashboard_data.py`)
- Combines sentiment and recommendation data
- Creates final datasets for dashboard consumption
- **Output:** `app_recommendations.parquet`, `final_recommendations.parquet`

## ğŸŒ Web Interfaces

### Streamlit Dashboard (Port 8501)
- Interactive game recommendations explorer
- Sentiment analysis visualizations
- Filtering and sorting capabilities
- Data export functionality

### FastAPI Service (Port 8000)
- RESTful API for recommendations
- Endpoints:
  - `GET /recommendations/{author_index}` - User recommendations
  - `GET /sentiment-leaderboard` - Top games by sentiment
  - `GET /games/{app_id}` - Game details
  - `GET /stats` - Overall statistics

### Airflow UI (Port 8080)
- Pipeline orchestration and monitoring
- DAG visualization and execution
- Task logs and debugging
- **Default login:** admin/admin

### MLflow UI (Port 5000)
- Experiment tracking and model versioning
- Metrics visualization
- Model artifact management

## ğŸ³ Docker Services

Airflow runs in Docker with the following services:
- **Webserver:** Airflow UI and API
- **Scheduler:** Task scheduling and execution
- **Worker:** Task execution (Celery)
- **PostgreSQL:** Metadata database
- **Redis:** Message broker

## ğŸ“ˆ Monitoring & Debugging

### HDFS Web UI
- **URL:** http://localhost:9870
- Monitor HDFS health, storage, and file system

### Spark UI
- Available during job execution
- Monitor stages, tasks, and performance

### Logs
- Airflow logs: `AIRFLOW-PROJECT/logs/`
- Spark logs: `$SPARK_HOME/logs/`
- Application logs: Console output

## ğŸ” Troubleshooting

### Common Issues

1. **HDFS not starting:**
```bash
# Check if namenode is formatted
ls ~/Hadoop/hdfs-data/namenode/current
# If empty, format namenode
$HADOOP_HOME/bin/hdfs namenode -format
```

2. **Spark jobs failing:**
```bash
# Check HDFS connectivity
hdfs dfs -ls /project/
# Verify Spark configuration
$SPARK_HOME/bin/spark-shell --conf spark.hadoop.fs.defaultFS=hdfs://localhost:9000
```

3. **Python dependencies:**
```bash
# Reinstall requirements
source .venv/bin/activate
pip install --upgrade -r requirements.txt
```

4. **Airflow connection issues:**
```bash
# Restart Airflow services
make airflow-down
make airflow-up
```

### Performance Tuning

- Adjust `spark.sql.shuffle.partitions` based on data size
- Increase Spark driver/executor memory for large datasets
- Tune ALS parameters (rank, iterations, regularization)
- Use appropriate HDFS replication factor

## ğŸ“ Development

### Adding New Pipeline Steps
1. Create new Python script in `pipeline/`
2. Add Spark session with HDFS configuration
3. Update `Makefile` with new target
4. Add task to Airflow DAG

### Extending the API
1. Add new endpoints to `serving/fastapi/main.py`
2. Update data models in Pydantic schemas
3. Test with FastAPI automatic docs at `/docs`

### Dashboard Enhancements
1. Modify `dashboards/GameSphere.py`
2. Add new Plotly visualizations
3. Extend filtering and interaction capabilities

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/new-feature`
3. Make changes and test locally
4. Commit changes: `git commit -am 'Add new feature'`
5. Push to branch: `git push origin feature/new-feature`
6. Submit pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Apache Spark and Hadoop communities
- Hugging Face Transformers library
- Streamlit and FastAPI frameworks
- Steam for providing review data access
